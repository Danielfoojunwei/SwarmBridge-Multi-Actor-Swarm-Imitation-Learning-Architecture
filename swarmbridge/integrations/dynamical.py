"""
Dynamical Integration Module

Bridge functions that make SwarmBridge outputs compatible with
Dynamical v0.3.3's skill-centric MoE architecture.

Key Functions:
- build_cooperative_skill_artifacts: Export trained models as Dynamical MoE experts
- register_skills_with_dynamical: Push artifacts to Dynamical skill registry
- load_global_prior_from_dynamical: Fetch global skill priors for distillation
"""

import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
import json
import onnx
import onnxruntime as ort

from swarmbridge.schemas.cooperative_skill_artifact import (
    CooperativeSkillArtifact,
    CooperativeSkillManifest,
    InputEmbeddingType,
    EncryptionScheme,
    CoordinationPrimitiveType,
    DynamicalCompatibilityMetadata,
)

logger = logging.getLogger(__name__)


def build_cooperative_skill_artifacts(
    model: nn.Module,
    role_configs: List[Dict[str, Any]],
    skill_id: str,
    moai_config: Dict[str, Any],
    output_dir: Path,
    coordination_primitive: str = "handover",
    version: str = "1.0",
    site_id: str = "default_site",
    round_id: int = 0,
    export_format: str = "onnx",  # or "pytorch"
) -> Dict[str, Any]:
    """
    Build Dynamical-compatible MoE skill expert artifacts from trained cooperative model.
    
    This is the critical bridge function that makes SwarmBridge outputs
    consumable by Dynamical's MoE layer.
    
    Args:
        model: Trained CooperativeBCModel
        role_configs: Role configuration (with skill_id, MOAI settings)
        skill_id: Skill ID from Dynamical skill catalog
        moai_config: MOAI configuration {"version": "0.3.3", "embedding_dim": 512}
        output_dir: Output directory for artifacts
        coordination_primitive: Type of coordination
        version: Skill version
        site_id: Training site identifier
        round_id: Federated learning round
        export_format: "onnx" or "pytorch"
    
    Returns:
        Dictionary with:
        - manifest: CooperativeSkillManifest
        - artifact_paths: Dict[role_id -> path]
        - onnx_models: Dict[role_id -> ONNX model] (if ONNX export)
    """
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"üî® Building Dynamical-compatible skill artifacts for {skill_id}")
    
    # Create manifest
    manifest = CooperativeSkillManifest(
        skill_id=skill_id,
        skill_name=skill_id.replace("_", " ").title(),
        coordination_primitive=CoordinationPrimitiveType(coordination_primitive),
        global_version=version,
        federated_sites=[site_id],
    )
    
    # Extract per-role experts from cooperative model
    role_artifacts = {}
    artifact_paths = {}
    onnx_models = {}
    
    model.eval()
    
    for role_config in role_configs:
        role_id = role_config["role_id"]
        
        logger.info(f"  Exporting role: {role_id}")
        
        # Extract role-specific policy
        if hasattr(model, 'policies') and role_id in model.policies:
            role_policy = model.policies[role_id]
        else:
            raise ValueError(f"Model doesn't have policy for role {role_id}")
        
        # Determine input embedding type
        if role_config.get("uses_moai", False):
            input_type = InputEmbeddingType.MOAI_512
        else:
            input_type = InputEmbeddingType.STATE_VECTOR
        
        # Export model
        if export_format == "onnx":
            # Export to ONNX for Dynamical's ONNX Runtime inference
            checkpoint_path = output_dir / f"{role_id}.onnx"
            
            # Create dummy input (MOAI embedding + coordination latent)
            obs_dim = role_config.get("observation_dim", 512)
            coord_dim = model.config.coordination_latent_dim
            
            dummy_obs = torch.randn(1, obs_dim)
            dummy_coord = torch.randn(1, coord_dim)
            
            # Export to ONNX
            torch.onnx.export(
                role_policy,
                (dummy_obs, dummy_coord),
                checkpoint_path,
                input_names=["observation", "coordination_latent"],
                output_names=["action"],
                dynamic_axes={
                    "observation": {0: "batch_size"},
                    "coordination_latent": {0: "batch_size"},
                    "action": {0: "batch_size"},
                },
                opset_version=14,
            )
            
            # Verify ONNX model
            onnx_model = onnx.load(str(checkpoint_path))
            onnx.checker.check_model(onnx_model)
            onnx_models[role_id] = onnx_model
            
            logger.info(f"    ‚úÖ Exported ONNX: {checkpoint_path}")
            
        elif export_format == "pytorch":
            # Export as PyTorch checkpoint
            checkpoint_path = output_dir / f"{role_id}.pt"
            
            torch.save({
                "model_state_dict": role_policy.state_dict(),
                "config": role_config,
                "coordination_latent_dim": model.config.coordination_latent_dim,
            }, checkpoint_path)
            
            logger.info(f"    ‚úÖ Exported PyTorch: {checkpoint_path}")
        
        else:
            raise ValueError(f"Unknown export format: {export_format}")
        
        artifact_paths[role_id] = checkpoint_path
        
        # Create artifact metadata
        artifact = CooperativeSkillArtifact(
            skill_id=skill_id,
            role_id=role_id,
            input_embedding_type=input_type,
            expert_checkpoint_uri=str(checkpoint_path),
            encryption_scheme=EncryptionScheme.NONE,  # Will encrypt separately
            version=version,
            site_id=site_id,
            round_id=round_id,
            coordination_primitive=CoordinationPrimitiveType(coordination_primitive),
            compatible_roles=[
                r["role_id"] for r in role_configs if r["role_id"] != role_id
            ],
            dynamical_compatibility=DynamicalCompatibilityMetadata(
                dynamical_version="0.3.3",
                moai_version=moai_config.get("version"),
                vla_base_model="pi0_7b",
                moe_layer_compatible=True,
                skill_catalog_verified=True,
                n2he_encryption_compatible=True,
            ),
            metadata={
                "export_format": export_format,
                "coordination_encoder_type": model.config.coordination_encoder_type,
                "coordination_latent_dim": model.config.coordination_latent_dim,
                "policy_hidden_dims": model.config.policy_hidden_dims,
            },
        )
        
        # Validate
        is_valid, msg = artifact.validate_dynamical_compatibility()
        if not is_valid:
            logger.warning(f"  ‚ö†Ô∏è  Artifact validation issues: {msg}")
        else:
            logger.info(f"    ‚úÖ Validated Dynamical compatibility")
        
        # Add to manifest
        manifest.add_role_artifact(artifact)
        role_artifacts[role_id] = artifact
        
        # Save individual artifact manifest
        artifact.to_json(output_dir / f"{role_id}_manifest.json")
    
    # Save combined manifest
    manifest_path = output_dir / "manifest.json"
    manifest.to_json(manifest_path)
    
    logger.info(f"‚úÖ Built {len(role_artifacts)} role artifacts for {skill_id}")
    logger.info(f"üìã Manifest saved to: {manifest_path}")
    
    return {
        "manifest": manifest,
        "artifact_paths": artifact_paths,
        "onnx_models": onnx_models if export_format == "onnx" else {},
        "role_artifacts": role_artifacts,
    }


def register_skills_with_dynamical(
    artifact_manifest: CooperativeSkillManifest,
    registry_uri: str,
    encryption_scheme: str = "n2he",
    api_key: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Register cooperative skill artifacts with Dynamical's cloud skill registry.
    
    This makes the trained skills available to all robots running Dynamical
    via the skill-centric API.
    
    Args:
        artifact_manifest: CooperativeSkillManifest with all role artifacts
        registry_uri: Dynamical registry URI (e.g., "dynamical://skills" or "https://registry.dynamical.ai")
        encryption_scheme: Encryption to apply ("n2he", "pyfhel", or "none")
        api_key: Optional API key for registry authentication
    
    Returns:
        Registration response with skill IDs and deployment info
    """
    
    logger.info(f"üì§ Registering skills with Dynamical registry: {registry_uri}")
    
    # Validate all artifacts
    is_valid, issues = artifact_manifest.validate_all_roles()
    if not is_valid:
        logger.error(f"‚ùå Artifact validation failed:")
        for issue in issues:
            logger.error(f"  - {issue}")
        raise ValueError("Artifact validation failed")
    
    # In production, this would:
    # 1. Encrypt artifacts with N2HE if needed
    # 2. Upload to Dynamical registry (S3/GCS + metadata DB)
    # 3. Update Dynamical skill catalog
    # 4. Trigger deployment to edge devices
    
    # For now, return mock response
    response = {
        "status": "registered",
        "skill_id": artifact_manifest.skill_id,
        "version": artifact_manifest.global_version,
        "registry_uri": registry_uri,
        "role_experts": {},
    }
    
    for role_id, artifact in artifact_manifest.role_artifacts.items():
        skill_key = artifact.get_dynamical_skill_key()
        
        response["role_experts"][role_id] = {
            "skill_key": skill_key,
            "checkpoint_uri": artifact.expert_checkpoint_uri,
            "dynamical_compatible": True,
            "moe_layer_ready": True,
        }
        
        logger.info(f"  ‚úÖ Registered {role_id}: {skill_key}")
    
    logger.info(f"‚úÖ Successfully registered {len(artifact_manifest.role_artifacts)} role experts")
    
    return response


def load_global_prior_from_dynamical(
    skill_id: str,
    version: str,
    registry_uri: str,
    api_key: Optional[str] = None,
) -> Dict[str, torch.Tensor]:
    """
    Load global skill prior from Dynamical registry for local-global distillation.
    
    This enables SwarmBridge to align local cooperative policies with the
    global fleet-wide skill priors maintained by Dynamical.
    
    Args:
        skill_id: Skill identifier
        version: Global version to load
        registry_uri: Dynamical registry URI
        api_key: Optional API key
    
    Returns:
        Dictionary of {role_id: global_prior_weights}
    """
    
    logger.info(f"üì• Loading global prior for {skill_id} v{version} from Dynamical")
    
    # In production, this would:
    # 1. Query Dynamical registry for global skill version
    # 2. Download encrypted weights
    # 3. Decrypt with shared keys
    # 4. Load into PyTorch tensors
    
    # For now, return placeholder
    logger.warning("‚ö†Ô∏è  Using placeholder global prior (production implementation needed)")
    
    return {
        "giver": {},
        "receiver": {},
    }


def encrypt_for_dynamical_n2he(
    checkpoint_path: Path,
    output_path: Path,
    key_id: str = "dynamical_fleet_key_v3",
) -> Path:
    """
    Encrypt model checkpoint using N2HE (Dynamical standard).
    
    Args:
        checkpoint_path: Path to unencrypted checkpoint
        output_path: Path to save encrypted checkpoint
        key_id: N2HE key identifier
    
    Returns:
        Path to encrypted checkpoint
    """
    
    logger.info(f"üîí Encrypting checkpoint with N2HE (key={key_id})")
    
    # In production, this would use actual N2HE library
    # For now, just copy (placeholder)
    
    import shutil
    output_path.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(checkpoint_path, output_path)
    
    logger.warning("‚ö†Ô∏è  Using placeholder encryption (production N2HE needed)")
    
    return output_path
